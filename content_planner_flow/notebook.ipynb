{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Planning and Publishing Crew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to create an AI crew for planning and publishing content using CrewAI Flows.\n",
    "The crew will take a link to blog post, download content as markdown using firecrawl, analyze it and generate a twitter thread and schedule it on Typefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and Setup\n",
    "Initial imports for the CrewAI Flow and Crew and setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshaypachaar/miniconda3/envs/env_crewai/lib/python3.10/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"schema\" in \"FirecrawlApp.ExtractParams\" shadows an attribute in parent \"BaseModel\"\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import getpass\n",
    "import os\n",
    "import datetime\n",
    "import uuid\n",
    "import yaml\n",
    "import json\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "\n",
    "# Firecrawl SDK\n",
    "from firecrawl import FirecrawlApp\n",
    "\n",
    "# Typefully scheduler\n",
    "from scheduler import schedule\n",
    "\n",
    "# Importing Crew related components\n",
    "from crewai import Agent, Task, Crew\n",
    "\n",
    "# Importing CrewAI Flow related components\n",
    "from crewai.flow.flow import Flow, listen, start\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Apply a patch to allow nested asyncio loops in Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blog Post URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_post_url = \"https://blog.dailydoseofds.com/p/5-chunking-strategies-for-rag\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan for our Flow\n",
    "\n",
    "1. Clone the repository for the project\n",
    "2. Plan the documentation for the project **[Crew of Agents]** \n",
    "3. Create the documentation for the project **[Crew of Agents]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![CrewAIFlow.png](crewai_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Planning Crew\n",
    "\n",
    "This structure will be used to capture the output of the planning crew which will be used to create the twitter thread and schedule it on Typefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet(BaseModel):\n",
    "    \"\"\"Represents an individual tweet in a thread\"\"\"\n",
    "    content: str\n",
    "    is_hook: bool = False  # Identifies if this is the opening/hook tweet\n",
    "    media_urls: Optional[list[str]] = []  # Optional media attachments (images, code snippets)\n",
    "\n",
    "class Thread(BaseModel):\n",
    "    \"\"\"Represents a Twitter thread\"\"\"\n",
    "    topic: str  # Main topic/subject of the thread\n",
    "    tweets: list[Tweet]  # List of tweets in the thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshaypachaar/miniconda3/envs/env_crewai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from crewai_tools import (\n",
    "    DirectoryReadTool,\n",
    "    FileReadTool,\n",
    ")\n",
    "\n",
    "# Load agent and task configurations from YAML files\n",
    "with open('config/planner_agents.yaml', 'r') as f:\n",
    "    agents_config = yaml.safe_load(f)\n",
    "\n",
    "with open('config/planner_tasks.yaml', 'r') as f:\n",
    "    tasks_config = yaml.safe_load(f)\n",
    "\n",
    "draft_analyzer = Agent(config=agents_config['draft_analyzer'], tools=[\n",
    "    DirectoryReadTool(),\n",
    "    FileReadTool()\n",
    "])\n",
    "twitter_thread_planner = Agent(config=agents_config['twitter_thread_planner'], tools=[\n",
    "    DirectoryReadTool(),\n",
    "    FileReadTool()\n",
    "])\n",
    "\n",
    "analyze_draft = Task(\n",
    "  config=tasks_config['analyze_draft'],\n",
    "  agent=draft_analyzer\n",
    ")\n",
    "create_twitter_thread_plan = Task(\n",
    "  config=tasks_config['create_twitter_thread_plan'],\n",
    "  agent=twitter_thread_planner,\n",
    "  output_pydantic=Thread\n",
    ")\n",
    "\n",
    "planning_crew = Crew(\n",
    "    agents=[draft_analyzer, twitter_thread_planner],\n",
    "    tasks=[analyze_draft, create_twitter_thread_plan],\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = planning_crew.kickoff(inputs={'draft_path': \"/Users/akshaypachaar/Eigen/ai-engineering/content_planner_flow/workdir/5_chunking_strategies_rag.md\"})\n",
    "# result.pydantic.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('output.json', 'w') as f:\n",
    "#     json.dump(result.pydantic.model_dump(), f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Documentation Flow\n",
    "\n",
    "A Flow to create the documentation for the project where we will use the planning crew to plan the documentation and the documentation crew to create the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentPlanningState(BaseModel):\n",
    "  \"\"\"\n",
    "  State for the content planning flow\n",
    "  \"\"\"\n",
    "  blog_post_url: str = blog_post_url\n",
    "  draft_path: Path = \"workdir/\"\n",
    "\n",
    "class CreateContentPlanningFlow(Flow[ContentPlanningState]):\n",
    "  # Scrape the blog post  \n",
    "  # No need for AI Agents on this step, so we just use regular Python code\n",
    "  @start()\n",
    "  def scrape_blog_post(self):\n",
    "    print(f\"# fetching draft from: {self.state.blog_post_url}\")\n",
    "    app = FirecrawlApp(api_key=os.getenv(\"FIRECRAWL_API_KEY\"))\n",
    "    scrape_result = app.scrape_url(self.state.blog_post_url, params={'formats': ['markdown', 'html']})\n",
    "    try:\n",
    "      title = scrape_result['metadata']['title']\n",
    "    except Exception as e:\n",
    "      title = str(uuid.uuid4())\n",
    "    self.state.draft_path = f'workdir/{title}.md'\n",
    "    with open(self.state.draft_path, 'w') as f:\n",
    "      f.write(scrape_result['markdown'])\n",
    "    return self.state\n",
    "\n",
    "  @listen(scrape_blog_post)\n",
    "  def plan_content(self):\n",
    "    print(f\"# Planning content for: {self.state.draft_path}\")\n",
    "    result = planning_crew.kickoff(inputs={'draft_path': self.state.draft_path})\n",
    "    print(f\"# Planned content for {self.state.draft_path}:\")\n",
    "    for tweet in result.pydantic.tweets:\n",
    "        print(f\"    - {tweet.content}\")\n",
    "    return result\n",
    "\n",
    "  @listen(plan_content)\n",
    "  def save_plan(self, plan):\n",
    "    with open(f'thread/{self.state.draft_path.split(\"/\")[-1]}.json', 'w') as f:\n",
    "        json.dump(plan.pydantic.model_dump(), f, indent=2)\n",
    "\n",
    "  @listen(plan_content)\n",
    "  def publish_thread(self, plan):\n",
    "    print(f\"# Publishing thread for: {self.state.draft_path}\")\n",
    "    ## Schedule for 1 hour from now    \n",
    "    response = schedule(\n",
    "        thread_json=plan\n",
    "    )\n",
    "    print(f\"# Thread scheduled for: {self.state.draft_path}\")\n",
    "    print(f\"Here's the link to scheduled draft: {response['url']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing helper methods to plot and execute the flow in a Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved as crewai_flow.html\n"
     ]
    }
   ],
   "source": [
    "# Plot the flow\n",
    "flow = CreateContentPlanningFlow()\n",
    "flow.plot()\n",
    "\n",
    "# Display the flow visualization using IFrame\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Display the flow visualization\n",
    "# IFrame(src='./crewai_flow.html', width='100%', height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# fetching draft from: https://blog.dailydoseofds.com/p/5-chunking-strategies-for-rag\n",
      "# Planning content for: workdir/5 Chunking Strategies For RAG - by Avi Chawla.md\n",
      "# Planned content for workdir/5 Chunking Strategies For RAG - by Avi Chawla.md:\n",
      "    - Chunking is key to RAG success!\n",
      "    - Retrieval-Augmented Generation (RAG) enhances the quality of AI-generated content by using external information effectively. Chunking large texts is essential for optimal performance in RAG workflows.\n",
      "    - 1Ô∏è‚É£ **Fixed-Size Chunking**: This basic method splits texts into uniform segments. While simple to implement, it often disrupts sentences and ideas due to fixed lengths. Consider overlapping sections to preserve context.\n",
      "    - 2Ô∏è‚É£ **Semantic Chunking**: Chunks are formed based on the semantic meaning of text. This way, language flow is maintained, improving retrieval accuracy. Implement embeddings and cosine similarity thresholds to form meaningful chunks.\n",
      "    - 3Ô∏è‚É£ **Recursive Chunking**: Start with paragraphs or sections and recursively split them if they exceed the size limit. This approach retains natural flow while creating manageable chunks.\n",
      "    - 4Ô∏è‚É£ **Document Structure-Based Chunking**: Using the document‚Äôs inherent structure‚Äîlike headings and sections‚Äîthis method maintains integrity while forming chunks. Ensure your document is well-structured for this approach to be effective.\n",
      "    - 5Ô∏è‚É£ **LLM-Based Chunking**: Utilize language models to generate semantically meaningful chunks. This method offers high accuracy but is computationally demanding. Balance your resources when considering this technique.\n",
      "    - üí° **Takeaway**: Assess each chunking strategy's pros and cons relative to your project needs. Testing different methods will lead to the most effective approach for your specific use case.\n",
      "    - üëâ What chunking strategies do you utilize? Share your thoughts and join the conversation!\n",
      "# Publishing thread for: workdir/5 Chunking Strategies For RAG - by Avi Chawla.md\n",
      "[Flow._execute_single_listener] Error in method publish_thread: schedule() got an unexpected keyword argument 'content'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/akshaypachaar/miniconda3/envs/env_crewai/lib/python3.10/site-packages/crewai/flow/flow.py\", line 363, in _execute_single_listener\n",
      "    listener_result = await self._execute_method(\n",
      "  File \"/Users/akshaypachaar/miniconda3/envs/env_crewai/lib/python3.10/site-packages/crewai/flow/flow.py\", line 306, in _execute_method\n",
      "    else method(*args, **kwargs)\n",
      "  File \"/var/folders/4r/7f58988s6cs3d64773nhcl1w0000gn/T/ipykernel_75586/3192203743.py\", line 45, in publish_thread\n",
      "    response = schedule(\n",
      "TypeError: schedule() got an unexpected keyword argument 'content'\n"
     ]
    }
   ],
   "source": [
    "flow = CreateContentPlanningFlow()\n",
    "flow.kickoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
